{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dset = pd.read_csv('liver_disease_.csv')\n",
    "features = dset.iloc[::, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode 'Gender' attribute\n",
    "le = LE()\n",
    "le.fit(['Male', 'Female'])\n",
    "features.Gender = le.transform(features.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle NaN data\n",
    "# replace missing-values with column mean\n",
    "for col in features.columns :\n",
    "    #if col  == 'Gender' : continue\n",
    "    mean = features[col].mean()\n",
    "    features[col] = features[col].fillna(mean)\n",
    "target = dset.iloc[::, -1]\n",
    "\n",
    "le.fit(['Yes', 'No'])\n",
    "target = le.transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SCALING\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
    "\n",
    "accuracy_all = {}\n",
    "f1_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelReport(modelName, yTest, yPredict) :\n",
    "    accuracy = accuracy_score(yTest, yPredict)\n",
    "    precision = precision_score(yTest, yPredict)\n",
    "    recall = recall_score(yTest, yPredict)\n",
    "    f1 = f1_score(yTest, yPredict)\n",
    "    conf_matrix = confusion_matrix(yTest, yPredict)\n",
    "    clf_report = classification_report(yTest, yPredict)\n",
    "    accuracy_all[modelName] = accuracy\n",
    "    f1_all[modelName] = f1\n",
    "\n",
    "    print('\\n\\n*************************************************************************************')\n",
    "    print('FOR {0} :'.format(modelName))\n",
    "    print('ACCURACY ->', accuracy)\n",
    "    print('CONFUSION MATRIX ->')\n",
    "    print(conf_matrix)\n",
    "    print('CLASSIFICATION REPORT ->')\n",
    "    print(clf_report)\n",
    "    print('*************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Implement AdaBoost ensemble Classifier with 3 different base classifiers (Logistic Regression, NaÃ¯ve Bayes and Polynomial SVM)\n",
    "\n",
    "# Implement AdaBoost with Logistic Regression as base classifier\n",
    "base_clf = LogisticRegression(solver='liblinear')\n",
    "model = AdaBoostClassifier(base_estimator=base_clf, n_estimators=30)\n",
    "model.fit(X_train, y_train)\n",
    "log_y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement AdaBoost with Naive Bayes as base classifier\n",
    "base_clf = GaussianNB()\n",
    "model = AdaBoostClassifier(base_estimator=base_clf, n_estimators=50, algorithm='SAMME')\n",
    "model.fit(X_train, y_train)\n",
    "nb_y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement AdaBoost with Polynomial SVM as base classifier\n",
    "#base_clf = SVC(kernel='poly', degree=3, class_weight='balanced', gamma='scale', probability=True)\n",
    "#base_clf = SVC(kernel='poly', degree=3, gamma='scale', probability=True)\n",
    "base_clf = SVC(kernel='poly', degree=8, gamma='scale')\n",
    "\n",
    "#model = AdaBoostClassifier(base_estimator=base_clf, n_estimators=10)#, algorithm='SAMME')\n",
    "model = AdaBoostClassifier(base_estimator=base_clf, algorithm='SAMME')\n",
    "model.fit(X_train, y_train)\n",
    "poly_y_predict = model.predict(X_test)\n",
    "#print(poly_y_predict[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Implement Random Forest (with Grid Search CV)\n",
    "parameters = {\n",
    "    'max_features' : ['auto', 2,3],\n",
    "    'criterion' : ['entropy', 'gini'],\n",
    "    'min_samples_split' : [2,3],\n",
    "    'min_samples_leaf' : [2,3],\n",
    "    'max_leaf_nodes' : [None,2,3],\n",
    "    'n_estimators' : [10, 50, 100]\n",
    "    }\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "grid_cv_forest = GridSearchCV(clf, param_grid=parameters, cv=5, iid=True)\n",
    "grid_cv_forest.fit(X_train, y_train)\n",
    "#print(grid_cv_forest.best_params_)\n",
    "best_params = grid_cv_forest.best_params_\n",
    "model = RandomForestClassifier(n_estimators = best_params['n_estimators'],\n",
    "                                max_features = best_params['max_features'],\n",
    "                                criterion = best_params['criterion'],\n",
    "                                min_samples_split = best_params['min_samples_split'],\n",
    "                                min_samples_leaf = best_params['min_samples_leaf'],\n",
    "                                max_leaf_nodes = best_params['max_leaf_nodes']\n",
    "                                )\n",
    "model.fit(X_train, y_train)\n",
    "forest_y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Implement XG Boost\n",
    "model = XGBClassifier(max_depth=2)\n",
    "model.fit(X_train, y_train)\n",
    "xgboost_y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************************************************************************************\n",
      "FOR ADABOOST WITH LOGISTIC REGRESSION AS BASE CLASSIFIER :\n",
      "ACCURACY -> 0.6971428571428572\n",
      "CONFUSION MATRIX ->\n",
      "[[  9  44]\n",
      " [  9 113]]\n",
      "CLASSIFICATION REPORT ->\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.17      0.25        53\n",
      "           1       0.72      0.93      0.81       122\n",
      "\n",
      "    accuracy                           0.70       175\n",
      "   macro avg       0.61      0.55      0.53       175\n",
      "weighted avg       0.65      0.70      0.64       175\n",
      "\n",
      "*************************************************************************************\n",
      "\n",
      "\n",
      "*************************************************************************************\n",
      "FOR ADABOOST WITH NAIVE BAYES AS BASE CLASSIFIER :\n",
      "ACCURACY -> 0.5714285714285714\n",
      "CONFUSION MATRIX ->\n",
      "[[53  0]\n",
      " [75 47]]\n",
      "CLASSIFICATION REPORT ->\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      1.00      0.59        53\n",
      "           1       1.00      0.39      0.56       122\n",
      "\n",
      "    accuracy                           0.57       175\n",
      "   macro avg       0.71      0.69      0.57       175\n",
      "weighted avg       0.82      0.57      0.57       175\n",
      "\n",
      "*************************************************************************************\n",
      "\n",
      "\n",
      "*************************************************************************************\n",
      "FOR ADABOOST WITH POLYNOMIAL SVM AS BASE CLASSIFIER :\n",
      "ACCURACY -> 0.6914285714285714\n",
      "CONFUSION MATRIX ->\n",
      "[[  0  53]\n",
      " [  1 121]]\n",
      "CLASSIFICATION REPORT ->\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        53\n",
      "           1       0.70      0.99      0.82       122\n",
      "\n",
      "    accuracy                           0.69       175\n",
      "   macro avg       0.35      0.50      0.41       175\n",
      "weighted avg       0.48      0.69      0.57       175\n",
      "\n",
      "*************************************************************************************\n",
      "\n",
      "\n",
      "*************************************************************************************\n",
      "FOR RANDOM FOREST (with Grid Search CV) :\n",
      "ACCURACY -> 0.68\n",
      "CONFUSION MATRIX ->\n",
      "[[ 14  39]\n",
      " [ 17 105]]\n",
      "CLASSIFICATION REPORT ->\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.26      0.33        53\n",
      "           1       0.73      0.86      0.79       122\n",
      "\n",
      "    accuracy                           0.68       175\n",
      "   macro avg       0.59      0.56      0.56       175\n",
      "weighted avg       0.65      0.68      0.65       175\n",
      "\n",
      "*************************************************************************************\n",
      "\n",
      "\n",
      "*************************************************************************************\n",
      "FOR XGBOOST :\n",
      "ACCURACY -> 0.6914285714285714\n",
      "CONFUSION MATRIX ->\n",
      "[[ 11  42]\n",
      " [ 12 110]]\n",
      "CLASSIFICATION REPORT ->\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.21      0.29        53\n",
      "           1       0.72      0.90      0.80       122\n",
      "\n",
      "    accuracy                           0.69       175\n",
      "   macro avg       0.60      0.55      0.55       175\n",
      "weighted avg       0.65      0.69      0.65       175\n",
      "\n",
      "*************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# d) Compare accuracy measures (Precision/Recall/F1/CM)\n",
    "getModelReport('ADABOOST WITH LOGISTIC REGRESSION AS BASE CLASSIFIER', y_test, log_y_predict)\n",
    "getModelReport('ADABOOST WITH NAIVE BAYES AS BASE CLASSIFIER', y_test, nb_y_predict)\n",
    "getModelReport('ADABOOST WITH POLYNOMIAL SVM AS BASE CLASSIFIER', y_test, poly_y_predict)\n",
    "getModelReport('RANDOM FOREST (with Grid Search CV)', y_test, forest_y_predict)\n",
    "getModelReport('XGBOOST', y_test, xgboost_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************************************************************************************\n",
      "ADABOOST WITH LOGISTIC REGRESSION AS BASE CLASSIFIER has the best accuracy score with value : 0.6971428571428572\n",
      "ADABOOST WITH POLYNOMIAL SVM AS BASE CLASSIFIER has the best f1 score with value : 0.8175675675675675\n"
     ]
    }
   ],
   "source": [
    "# find model with best accuracy and best f1\n",
    "acc_best = 0\n",
    "model_acc_best = None\n",
    "f1_best = 0\n",
    "\n",
    "for model in accuracy_all :\n",
    "    if accuracy_all[model] > acc_best :\n",
    "        model_acc_best = model\n",
    "        acc_best = accuracy_all[model]\n",
    "\n",
    "for model in f1_all :\n",
    "    if f1_all[model] > f1_best :\n",
    "        model_f1_best = model\n",
    "        f1_best = f1_all[model]\n",
    "\n",
    "print('\\n\\n*************************************************************************************')\n",
    "print('{0} has the best accuracy score with value : {1}'.format(model_acc_best, accuracy_all[model_acc_best]))\n",
    "print('{0} has the best f1 score with value : {1}'.format(model_f1_best, f1_all[model_f1_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
