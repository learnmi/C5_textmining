{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><font color='blue'>Topic Modelling using LDA</font></center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, we learn topic modelling using LDA by create a LDA model. We use PorterStemmer while building the model.\n",
    "##### Please refer the API Documentation Here : https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sonu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'until', 'll', \"aren't\", 'but', 'their', 'themselves', 'such', 'had', 'i', 'to', 'some', 'as', 'myself', 'what', 'having', 's', 'o', 'for', 'from', 'with', 'during', 'your', 'shan', 'no', 'yourself', 'only', 'shouldn', \"shouldn't\", 'm', 're', 'y', 'my', 'then', 'it', 'and', 'into', 'between', 'after', 'ours', 'has', \"don't\", \"she's\", 'itself', 'because', 'yours', 'was', 'once', 'wasn', \"it's\", 'than', 'were', 'herself', 'wouldn', 'won', \"isn't\", 'up', 'below', 'own', 'theirs', 'our', 'isn', 'mightn', \"shan't\", 'again', \"you'll\", 'against', 'ain', 'his', 'through', 'doesn', 'out', 'of', \"mustn't\", 'they', 'are', 'been', 'why', \"wouldn't\", 'have', 'other', 'before', 'didn', 'on', 'mustn', 'few', 'or', 'while', 'her', 'ourselves', 'that', 'not', 'here', 'how', \"you're\", \"weren't\", 'yourselves', \"won't\", 'this', 'where', 'don', 'most', 'too', 'hadn', 'weren', 'should', 'both', 'ma', 'hers', 'them', 'will', 't', 'when', \"should've\", 'is', 'about', 'any', \"wasn't\", 'more', 'if', \"mightn't\", 'there', 'those', 'who', 'haven', 'we', \"you've\", 'all', 'just', 'me', 'couldn', \"haven't\", 'these', 'am', 'now', \"doesn't\", 'aren', \"couldn't\", 'does', 'over', \"you'd\", 'd', 'by', 'off', 'hasn', \"that'll\", 'further', 'needn', 've', \"hadn't\", 'in', 'the', 'its', 'being', 'an', 'at', 'nor', 'did', 'can', \"needn't\", 'you', 'under', 'doing', 'each', \"hasn't\", 'she', 'which', 'he', 'himself', 'a', 'down', 'same', 'above', 'be', 'do', 'whom', 'so', 'very', \"didn't\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# from gensim import corpora, models\n",
    "# import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "print(en_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating English stop words list and PorterStemmer Class --> 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the LDA Model --> 2.5 M "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "************* Tasks TODO ******************************************\n",
    "(1) Remove stop words from tokens\n",
    "(2) Stem tokens using p_stemmer\n",
    "(3) Create dictionary - Hint : Use corpora.Dictionary()\n",
    "(4) Convert Tokenized document into Document - term Matrix!\n",
    "(5) Generate LDAmodel - Hint : Explore about gensim.models.ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens -------- (1) \n",
    "    stopped_tokens = # Code here ... \n",
    "    \n",
    "    # stem tokens ------ (2)\n",
    "    stemmed_tokens = # Code here ... \n",
    "    \n",
    "    # add stemmed_tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary ------ (3)\n",
    "dictionary = # Code here ... \n",
    "    \n",
    "# convert tokenized documents into a document-term matrix ----- (4)\n",
    "corpus = # Code here ...\n",
    "\n",
    "# generate LDA model -------- (5)\n",
    "ldamodel = # Code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the ldamodel with print_topics by using different input values -- 0.5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.086*\"health\" + 0.086*\"good\" + 0.086*\"brocolli\" + 0.061*\"eat\"'), (1, '0.068*\"mother\" + 0.068*\"brother\" + 0.068*\"drive\" + 0.041*\"pressur\"')]\n"
     ]
    }
   ],
   "source": [
    "print( # Code here ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.086*\"health\" + 0.086*\"good\" + 0.086*\"brocolli\"'), (1, '0.068*\"mother\" + 0.068*\"brother\" + 0.068*\"drive\"')]\n"
     ]
    }
   ],
   "source": [
    "print( # Code here ... )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
